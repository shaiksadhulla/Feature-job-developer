#importing data
import numpy as np
import pandas as pd
url="https://raw.githubusercontent.com/Nantha-1998/404-CODER/master/career_compute_dataset.csv"
dataset = pd.read_csv(url)
print(np.shape(dataset))
dataset.head()
data = dataset.iloc[:49,:-1].values
label = dataset.iloc[:49,-1]
#-------------------------------Label Encoding--------------------------#
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
labelencoder = LabelEncoder()
df = dataset
label = df.iloc[:49,-1]
original=label.unique()
label=label.values
label2 = labelencoder.fit_transform(label)
y=pd.DataFrame(label2,columns=["ROLE"])
numeric=y["ROLE"].unique()
y1 = pd.DataFrame({'ROLE':original, 'Associated Number':numeric})
print(y1)
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
labelencoder = LabelEncoder()
label = labelencoder.fit_transform(label)
y=pd.DataFrame(label,columns=["role"])
X =pd.DataFrame(data,columns=['sslc','hsc','cgpa','school_type','no_of_miniprojects','no_of_projects',
'coresub_skill','aptitude_skill','problemsolving_skill','programming_skill','abs'
'tractthink_skill',
'design_skill','first_computer','first_program','lab_programs','ds_coding','technology_used',
'sympos_attend','sympos_won','extracurricular','learning_style','college_bench','clg_teachers_know','college_performence','college_skills'])
from pandas import read_csv
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OrdinalEncoder
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
from matplotlib import pyplot
# feature selection
# example of chi squared feature selection for categorical data
def select_features(X_train, y_train, X_test):
    fs = SelectKBest(score_func=chi2, k='all')
    fs.fit(X_train, y_train)
    X_train_fs = fs.transform(X_train)
    X_test_fs = fs.transform(X_test)
    return X_train_fs, X_test_fs, fs
# split into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=20)#Decisiontree
X_train2,X_test2,y_train2,y_test2=train_test_split(X,y,test_size=0.3,random_state=10)#XGBoost
X_train6,X_test6,y_train6,y_test6=train_test_split(X,y,test_size=0.2,random_state=15)#SVM
X_train1, X_test1, fs1 = select_features(X_train, y_train, X_test)
X_train3, X_test3, fs3 = select_features(X_train2, y_train2, X_test2)
X_train5, X_test5, fs5 = select_features(X_train2, y_train2, X_test2)
# what are scores for the features
for i in range(len(fs1.scores_)):
    print('Feature %d: %f' % (i, fs1.scores_[i]))
# plot the scores
pyplot.bar([i for i in range(len(fs1.scores_))], fs1.scores_)
pyplot.show()
count = 0
for i in fs1.scores_ :
    if i > 1 :
        count = count + 1
k1=count
print(" The number of important features with threshold as 1 :{} ".format(k1))
for i in range(len(fs3.scores_)):
    print('Feature %d: %f' % (i, fs3.scores_[i]))
# plot the scores
pyplot.bar([i for i in range(len(fs3.scores_))], fs3.scores_)
pyplot.show()
count = 0
for i in fs3.scores_ :
    if i > 1 :
        count = count + 1
k3=count
print(" The number of important features with threshold as 1 :{} ".format(k3))
#X_train4, X_test4, fs = select_features(X_train2, y_train2, X_test2)
for i in range(len(fs5.scores_)):
    print('Feature %d: %f' % (i, fs5.scores_[i]))
# plot the scores
pyplot.bar([i for i in range(len(fs5.scores_))], fs5.scores_)
pyplot.show()
count = 0
for i in fs5.scores_ :
    if i > 1 :
        count = count + 1
k5=count
print(" The number of important features with threshold as 1 : {}" .format(k5))
#Support Vector machine
from sklearn.metrics import confusion_matrix,accuracy_score
def svm(X_train,y_train,X_test,y_test):
    from sklearn.svm import SVC
    from pandas import read_csv
    from sklearn.model_selection import train_test_split
    # import metrics to compute accuracy
    from sklearn.metrics import accuracy_score
    svc=SVC()
# fit classifier to training set
    svc.fit(X_train,y_train)
# make predictions on test set
    y_pred=svc.predict(X_test)
# compute and print accuracy score
    print('Model accuracy score with default hyperparameters: {0:0.4f}'.
    format(accuracy_score(y_test, y_pred)*100))
    return accuracy_score(y_test, y_pred)*100
def Dec_tree(X_train,y_train,X_test,y_test):
    from sklearn import tree
    clf = tree.DecisionTreeClassifier()
    clf = clf.fit(X_train, y_train)
    # Prediction
    y_pred = clf.predict(X_test)
    y_test_arr=y_test['role']
    from sklearn.metrics import confusion_matrix,accuracy_score
    accuracy = accuracy_score(y_test,y_pred)
    print('Model accuracy score with Decision Tree', accuracy_score(y_test, y_pred)*100)
    return accuracy*100,clf
def xgboost(X_train,y_train,X_test,y_test,clf):
    #X_train,X_test,y_train,y_test=train_test_split(X1,y,test_size=0.3,random_state=10)
    shape = X_train.shape
    X_train=pd.to_numeric(X_train.values.flatten())
    X_train=X_train.reshape(shape)
    from xgboost.sklearn import XGBClassifier
    model = XGBClassifier()
    model.fit(X_train, y_train)
    xgb_y_pred = clf.predict(X_test)
    xgb_accuracy = accuracy_score(y_test,xgb_y_pred)
    print("accuracy=",xgb_accuracy*100)
    return xgb_accuracy*100
def select_features(X_train, y_train, X_test):
    fs = SelectKBest(score_func=chi2,k=k1)
    fs.fit(X_train, y_train)
    X_train_fs = fs.transform(X_train)
    X_test_fs = fs.transform(X_test)
    return X_train_fs, X_test_fs, fs
def select_features2(X_train, y_train, X_test):
    fs = SelectKBest(score_func=chi2,k=k3)
    fs.fit(X_train, y_train)
    X_train_fs = fs.transform(X_train)
    X_test_fs = fs.transform(X_test)
    return X_train_fs, X_test_fs, fs
def select_features3(X_train, y_train, X_test):
    fs = SelectKBest(score_func=chi2,k=k5)
    fs.fit(X_train, y_train)
    X_train_fs = fs.transform(X_train)
    X_test_fs = fs.transform(X_test)
    return X_train_fs, X_test_fs, fs
X_train1, X_test1, fs = select_features(X_train, y_train, X_test)
X_train3, X_test3, fs = select_features2(X_train2, y_train2, X_test2)
X_train5, X_test5, fs = select_features3(X_train6, y_train6, X_test6)
#---------------SVM------------------#
#
print("Without feature Selection : ")
acc = svm(X_train6,y_train6,X_test6,y_test6)
print("With feature Selection : ")
acc1 = svm(X_train5,y_train6,X_test5,y_test6)
#-------------Decision Tree-----------------#
print("Without feature Selection : ")
acc1,clf1 = Dec_tree(X_train1,y_train,X_test1,y_test)
print("With feature Selection : ")
acc,clf = Dec_tree(X_train,y_train,X_test,y_test)
print(' ')
#-------------XGBoost-----------------#
print("Boosting the Decision Tree ")
acc = xgboost(X_train2,y_train2,X_test2,y_test2,clf)
# example of mutual information feature selection for categorical data
from sklearn.feature_selection import mutual_info_classif
def select_features(X_train, y_train, X_test):
    fs = SelectKBest(score_func=mutual_info_classif, k='all')
    fs.fit(X_train, y_train)
    X_train_fs = fs.transform(X_train)
    X_test_fs = fs.transform(X_test)
    return X_train_fs, X_test_fs, fs
# split into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=20)#Decisiontree
X_train2,X_test2,y_train2,y_test2=train_test_split(X,y,test_size=0.3,random_state=10)#XGBoost
X_train6,X_test6,y_train6,y_test6=train_test_split(X,y,test_size=0.2,random_state=15)#SVM
X_train1, X_test1, fs1 = select_features(X_train, y_train, X_test)
X_train3, X_test3, fs3 = select_features(X_train2, y_train2, X_test2)
X_train5, X_test5, fs5 = select_features(X_train2, y_train2, X_test2)
# what are scores for the features
for i in range(len(fs1.scores_)):
    print('Feature %d: %f' % (i, fs.scores_[i]))
    # plot the scores
    pyplot.bar([i for i in range(len(fs.scores_))], fs.scores_)
    pyplot.show()
    count = 0
    for i in fs.scores_ :
        if i > 0.2 :
            count = count + 1
            k1=count
    print(" The number of important features with threshold as 0.2 :{} ".format(k1))
for i in range(len(fs3.scores_)):
    print('Feature %d: %f' % (i, fs3.scores_[i]))
# plot the scores
pyplot.bar([i for i in range(len(fs3.scores_))], fs3.scores_)
pyplot.show()
count = 0
for i in fs3.scores_ :
    if i > 0.2 :
        count = count + 1
        k3=count
print(" The number of important features with threshold as 0.2 :{} ".format(k3))
#X_train4, X_test4, fs = select_features(X_train2, y_train2, X_test2)
for i in range(len(fs5.scores_)):
    print('Feature %d: %f' % (i, fs5.scores_[i]))
# plot the scores
    pyplot.bar([i for i in range(len(fs5.scores_))], fs5.scores_)
    pyplot.show()
    count = 0
for i in fs5.scores_ :
    if i > 0.2 :
        count = count + 1
        k5=count
print(" The number of important features with threshold as 0.2 : {}" .format(k5))
def select_features(X_train, y_train, X_test):
    fs = SelectKBest(score_func=mutual_info_classif,k=k1)
    fs.fit(X_train, y_train)
    X_train_fs = fs.transform(X_train)
    X_test_fs = fs.transform(X_test)
    return X_train_fs, X_test_fs, fs
def select_features2(X_train, y_train, X_test):
    fs = SelectKBest(score_func=mutual_info_classif,k=k3)
    fs.fit(X_train, y_train)
    X_train_fs = fs.transform(X_train)
    X_test_fs = fs.transform(X_test)
    return X_train_fs, X_test_fs, fs
def select_features3(X_train, y_train, X_test):
    fs = SelectKBest(score_func=mutual_info_classif,k=k5)
    fs.fit(X_train, y_train)
    X_train_fs = fs.transform(X_train)
    X_test_fs = fs.transform(X_test)
    return X_train_fs, X_test_fs, fs
X_train1, X_test1, fs = select_features(X_train, y_train, X_test)
X_train3, X_test3, fs = select_features2(X_train2, y_train2, X_test2)
X_train5, X_test5, fs = select_features3(X_train6, y_train6, X_test6)
#---------------SVM------------------#
#
print("Without feature Selection : ")
acc = svm(X_train6,y_train6,X_test6,y_test6)
print("With feature Selection : ")
acc1 = svm(X_train5,y_train6,X_test5,y_test6)
#-------------Decision Tree-----------------#
print("Without feature Selection : ")
acc1,clf1 = Dec_tree(X_train1,y_train,X_test1,y_test)
print("With feature Selection : ")
acc,clf = Dec_tree(X_train,y_train,X_test,y_test)
print(' ')
#-------------XGBoost-----------------#
print("Boosting the Decision Tree ")
acc = xgboost(X_train2,y_train2,X_test2,y_test2,clf)
import matplotlib.pyplot as plt
#fig = plt.figure()
fig = plt.figure(figsize =(10, 5))
#ax = fig.add_axes([0,0,1,1])
Name = ['Decision Tree(with feature selection)','Decision Tree(without feature selection)',
'XGBoost', 'SVM(with feature selction)','SVM(without feature selction)']
accuracies = [50,40,93.333,70,60]
plt.bar(Name,accuracies)
plt.ylabel('Accuracies')
plt.xlabel('Techniques')
plt.show()
# x_new = ['3','5','4','3','5','4','3','5','2','2','5','2','5','4','2','5','5','3','2','5','5','4','2','3','4']
x_new = []
feat = list(X.columns)
for i in feat:
    print(i)
val = input("Enter: ")
x_new.append(val)
x_new
x_new = ['3','5','4','3','5','4','3','5','2','2','5','2','5','4','2','5','5','3','2','5','5','4','2','3','4']
new_pred = clf.predict([x_new])
print("Prediction : {}".format(y1[y1['Associated Number']==new_pred[0]]['ROLE']))

